"""
Import API Endpoints

ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚
URLã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å‡¦ç†æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import logging
import json
import asyncio
from typing import Any, Dict, Optional
from uuid import uuid4
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, Query, Path, BackgroundTasks
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.core.deps import get_current_user
from app.schemas.import_schema import (
    URLImportRequest,
    FileImportRequest,
    ImportResponse,
    ImportStatusResponse,
    ImportResultDetail,
    ImportListResponse,
    ImportType,
    ImportStatus
)
from app.services.file_processor import file_processor, FileProcessorError
from app.services.url_importer import url_importer, URLImportError
from app.services.ai.service import AIService
from app.providers.storage import get_storage_provider

logger = logging.getLogger(__name__)

router = APIRouter()

# é€²è¡Œä¸­ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã‚’ç®¡ç†ã™ã‚‹è¾æ›¸ï¼ˆæœ¬æ¥ã¯Redisç­‰ã§ç®¡ç†ï¼‰
import_jobs: Dict[str, Dict[str, Any]] = {}


@router.post("/url", response_model=ImportResponse)
async def import_from_url(
    *,
    request: URLImportRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Any:
    """
    URLã‹ã‚‰ãƒãƒ¼ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    
    - **url**: ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ã®URLï¼ˆWebãƒšãƒ¼ã‚¸ã€YouTubeç­‰ï¼‰
    - **extract_options**: æŠ½å‡ºã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå­—å¹•è¨€èªç­‰ï¼‰
    - **auto_title**: AIã«ã‚ˆã‚‹è‡ªå‹•ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
    - **auto_split**: è‡ªå‹•ãƒšãƒ¼ã‚¸åˆ†å‰²
    """
    try:
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆIDã‚’ç”Ÿæˆ
        import_id = str(uuid4())
        
        # URLå½¢å¼ã®ãƒã‚§ãƒƒã‚¯
        if not url_importer.is_url_supported(str(request.url)):
            raise HTTPException(
                status_code=400,
                detail="ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„URLå½¢å¼ã§ã™"
            )
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¸ãƒ§ãƒ–ã‚’ç™»éŒ²
        job_info = {
            "import_id": import_id,
            "import_type": ImportType.URL,
            "status": ImportStatus.PENDING,
            "progress": 0.0,
            "source_info": {
                "url": str(request.url),
                "auto_title": request.auto_title,
                "auto_split": request.auto_split,
                "extract_options": request.extract_options
            },
            "user_id": current_user["uid"],
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
        import_jobs[import_id] = job_info
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        background_tasks.add_task(
            _process_url_import,
            import_id,
            str(request.url),
            request.extract_options,
            request.auto_title,
            request.auto_split,
            current_user["uid"]
        )
        
        logger.info(f"URL import started: {import_id}, URL: {request.url}")
        
        return ImportResponse(
            import_id=import_id,
            import_type=ImportType.URL,
            status=ImportStatus.PENDING,
            source_info=job_info["source_info"],
            created_at=job_info["created_at"],
            estimated_completion_time=30  # 30ç§’ç¨‹åº¦ã‚’æƒ³å®š
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting URL import: {e}")
        raise HTTPException(
            status_code=500,
            detail="URLã‚¤ãƒ³ãƒãƒ¼ãƒˆã®é–‹å§‹ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
        )


@router.post("/file", response_model=ImportResponse)
async def import_from_file(
    *,
    request: FileImportRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Any:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒãƒ¼ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    
    - **media_id**: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ãƒ‡ã‚£ã‚¢ID
    - **extract_options**: æŠ½å‡ºã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç­‰ï¼‰
    - **auto_title**: AIã«ã‚ˆã‚‹è‡ªå‹•ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
    - **auto_split**: è‡ªå‹•ãƒšãƒ¼ã‚¸åˆ†å‰²
    """
    try:
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆIDã‚’ç”Ÿæˆ
        import_id = str(uuid4())
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢IDã®å­˜åœ¨ç¢ºèªï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çµŒç”±ï¼‰
        storage_provider = get_storage_provider()
        
        try:
            # ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±ã®å–å¾—ã‚’è©¦è¡Œ
            media_status = await storage_provider.get_media_status(
                request.media_id,
                current_user["uid"]
            )
            
            if media_status.get("status") != "completed":
                raise HTTPException(
                    status_code=400,
                    detail="ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“"
                )
                
        except Exception as e:
            logger.warning(f"Could not verify media status for {request.media_id}: {e}")
            # ãƒ¡ãƒ‡ã‚£ã‚¢çŠ¶æ³ãŒå–å¾—ã§ããªã„å ´åˆã¯å‡¦ç†ã‚’ç¶šè¡Œ
            pass
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¸ãƒ§ãƒ–ã‚’ç™»éŒ²
        job_info = {
            "import_id": import_id,
            "import_type": ImportType.FILE,
            "status": ImportStatus.PENDING,
            "progress": 0.0,
            "source_info": {
                "media_id": request.media_id,
                "auto_title": request.auto_title,
                "auto_split": request.auto_split,
                "extract_options": request.extract_options
            },
            "user_id": current_user["uid"],
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
        import_jobs[import_id] = job_info
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        background_tasks.add_task(
            _process_file_import,
            import_id,
            request.media_id,
            request.extract_options,
            request.auto_title,
            request.auto_split,
            current_user["uid"]
        )
        
        logger.info(f"File import started: {import_id}, Media ID: {request.media_id}")
        
        return ImportResponse(
            import_id=import_id,
            import_type=ImportType.FILE,
            status=ImportStatus.PENDING,
            source_info=job_info["source_info"],
            created_at=job_info["created_at"],
            estimated_completion_time=60  # 60ç§’ç¨‹åº¦ã‚’æƒ³å®š
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting file import: {e}")
        raise HTTPException(
            status_code=500,
            detail="ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®é–‹å§‹ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
        )


@router.get("/status/{import_id}", response_model=ImportStatusResponse)
async def get_import_status(
    *,
    import_id: str = Path(..., description="ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ID"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Any:
    """
    ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã®çŠ¶æ³ã‚’å–å¾—
    
    - **import_id**: ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ID
    """
    if import_id not in import_jobs:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        )
    
    job_info = import_jobs[import_id]
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨©é™ãƒã‚§ãƒƒã‚¯
    if job_info["user_id"] != current_user["uid"]:
        raise HTTPException(
            status_code=403,
            detail="ã“ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“"
        )
    
    return ImportStatusResponse(
        import_id=import_id,
        import_type=job_info["import_type"],
        status=job_info["status"],
        progress=job_info.get("progress", 0.0),
        note_id=job_info.get("note_id"),
        total_pages=job_info.get("total_pages"),
        extracted_text_length=job_info.get("extracted_text_length"),
        title=job_info.get("title"),
        error_message=job_info.get("error_message"),
        error_code=job_info.get("error_code"),
        created_at=job_info["created_at"],
        updated_at=job_info["updated_at"],
        completed_at=job_info.get("completed_at")
    )


@router.get("/result/{import_id}", response_model=ImportResultDetail)
async def get_import_result(
    *,
    import_id: str = Path(..., description="ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ID"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Any:
    """
    ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã®è©³ç´°çµæœã‚’å–å¾—
    
    - **import_id**: ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ID
    """
    if import_id not in import_jobs:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        )
    
    job_info = import_jobs[import_id]
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨©é™ãƒã‚§ãƒƒã‚¯
    if job_info["user_id"] != current_user["uid"]:
        raise HTTPException(
            status_code=403,
            detail="ã“ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“"
        )
    
    # å‡¦ç†å®Œäº†ãƒã‚§ãƒƒã‚¯
    if job_info["status"] != ImportStatus.COMPLETED:
        raise HTTPException(
            status_code=400,
            detail="ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“"
        )
    
    return ImportResultDetail(
        note_id=job_info["note_id"],
        title=job_info["title"],
        total_pages=job_info["total_pages"],
        pages=job_info.get("pages", []),
        source_metadata=job_info.get("source_metadata", {}),
        extraction_metadata=job_info.get("extraction_metadata", {}),
        processing_time=job_info.get("processing_time", 0.0),
        created_at=job_info["created_at"]
    )


@router.get("/history", response_model=ImportListResponse)
async def get_import_history(
    *,
    current_user: Dict[str, Any] = Depends(get_current_user),
    page: int = Query(1, ge=1, description="ãƒšãƒ¼ã‚¸ç•ªå·"),
    page_size: int = Query(20, ge=1, le=100, description="ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º"),
    import_type: Optional[ImportType] = Query(None, description="ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿"),
    status: Optional[ImportStatus] = Query(None, description="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ãƒ•ã‚£ãƒ«ã‚¿")
) -> Any:
    """
    ã‚¤ãƒ³ãƒãƒ¼ãƒˆå±¥æ­´ã‚’å–å¾—
    
    - **page**: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1ã‹ã‚‰é–‹å§‹ï¼‰
    - **page_size**: ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºï¼ˆæœ€å¤§100ï¼‰
    - **import_type**: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿
    - **status**: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ãƒ•ã‚£ãƒ«ã‚¿
    """
    user_id = current_user["uid"]
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå±¥æ­´ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    user_imports = [
        job for job in import_jobs.values()
        if job["user_id"] == user_id
    ]
    
    # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
    if import_type:
        user_imports = [job for job in user_imports if job["import_type"] == import_type]
    
    if status:
        user_imports = [job for job in user_imports if job["status"] == status]
    
    # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    user_imports.sort(key=lambda x: x["created_at"], reverse=True)
    
    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
    total = len(user_imports)
    start_idx = (page - 1) * page_size
    end_idx = start_idx + page_size
    page_items = user_imports[start_idx:end_idx]
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
    items = [
        ImportStatusResponse(
            import_id=job["import_id"],
            import_type=job["import_type"],
            status=job["status"],
            progress=job.get("progress", 0.0),
            note_id=job.get("note_id"),
            total_pages=job.get("total_pages"),
            extracted_text_length=job.get("extracted_text_length"),
            title=job.get("title"),
            error_message=job.get("error_message"),
            error_code=job.get("error_code"),
            created_at=job["created_at"],
            updated_at=job["updated_at"],
            completed_at=job.get("completed_at")
        ) for job in page_items
    ]
    
    return ImportListResponse(
        items=items,
        total=total,
        page=page,
        page_size=page_size,
        has_next=end_idx < total
    )


# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–¢æ•°
async def _process_url_import(
    import_id: str,
    url: str,
    extract_options: Dict[str, Any],
    auto_title: bool,
    auto_split: bool,
    user_id: str
):
    """URLã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†"""
    job_info = import_jobs[import_id]
    
    try:
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ï¼šå‡¦ç†ä¸­
        job_info.update({
            "status": ImportStatus.PROCESSING,
            "progress": 0.1,
            "updated_at": datetime.utcnow()
        })
        
        # URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        logger.info(f"Starting URL extraction for import {import_id}")
        extraction_result = await url_importer.extract_text_from_url(url, extract_options)
        
        job_info.update({
            "progress": 0.5,
            "updated_at": datetime.utcnow()
        })
        
        # AIã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        title = extraction_result.get('title', 'Imported Note')
        if auto_title and extraction_result.get('text'):
            try:
                ai_service = AIService()
                title = await ai_service.generate_title(extraction_result['text'][:500])
                logger.info(f"AI title generated for import {import_id}: {title}")
            except Exception as e:
                logger.warning(f"AI title generation failed for import {import_id}: {e}")
                title = extraction_result.get('title', 'Imported Note')
        
        job_info.update({
            "progress": 0.7,
            "updated_at": datetime.utcnow()
        })
        
        # ãƒãƒ¼ãƒˆä½œæˆï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸å¯¾å¿œï¼‰
        note_id = f"import_{import_id}"
        extracted_text = extraction_result['text']
        
        # ğŸ†• Phase 3: Feature Flagå¯¾å¿œã®ãƒšãƒ¼ã‚¸åˆ†å‰²å‡¦ç†
        pages = []
        from app.core.settings import settings
        
        if auto_split and len(extracted_text) > 2000 and settings.IMPORT_SPLIT_ENABLED:
            # ğŸ†• Phase 1ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½ã‚’ä½¿ç”¨
            from app.services.url_importer import _split_into_chunks
            text_chunks = _split_into_chunks(extracted_text, max_chars=2000)
            logger.info(f"Split text into {len(text_chunks)} chunks for import {import_id}")
            
            for i, chunk in enumerate(text_chunks):
                pages.append({
                    "page_number": i + 1,
                    "text": chunk,
                    "text_length": len(chunk),
                    "is_ai_enhanced": False  # AIæ•´å½¢ã¯å¾Œã§å®Ÿè£…
                })
        else:
            # å¾“æ¥ã®1ãƒšãƒ¼ã‚¸å½¢å¼ï¼ˆFeature Flag OFF ã¾ãŸã¯ 2000æ–‡å­—ä»¥ä¸‹ï¼‰
            pages.append({
                "page_number": 1,
                "text": extracted_text,
                "text_length": len(extracted_text),
                "is_ai_enhanced": False
            })
        
        # å‡¦ç†å®Œäº†
        job_info.update({
            "status": ImportStatus.COMPLETED,
            "progress": 1.0,
            "note_id": note_id,
            "title": title,
            "total_pages": len(pages),
            "pages": pages,
            "extracted_text_length": len(extracted_text),
            "source_metadata": extraction_result.get('metadata', {}),
            "extraction_metadata": {
                "source_type": extraction_result.get('source_type'),
                "extraction_time": datetime.utcnow().isoformat()
            },
            "processing_time": (datetime.utcnow() - job_info["created_at"]).total_seconds(),
            "updated_at": datetime.utcnow(),
            "completed_at": datetime.utcnow()
        })
        
        logger.info(f"URL import completed successfully: {import_id}")
        
    except URLImportError as e:
        logger.error(f"URL import error for {import_id}: {e}")
        job_info.update({
            "status": ImportStatus.FAILED,
            "error_message": str(e),
            "error_code": "URL_IMPORT_ERROR",
            "updated_at": datetime.utcnow(),
            "completed_at": datetime.utcnow()
        })
        
    except Exception as e:
        logger.error(f"Unexpected error in URL import {import_id}: {e}")
        job_info.update({
            "status": ImportStatus.FAILED,
            "error_message": "äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            "error_code": "INTERNAL_ERROR",
            "updated_at": datetime.utcnow(),
            "completed_at": datetime.utcnow()
        })


async def _process_file_import(
    import_id: str,
    media_id: str,
    extract_options: Dict[str, Any],
    auto_title: bool,
    auto_split: bool,
    user_id: str
):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†"""
    job_info = import_jobs[import_id]
    
    try:
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ï¼šå‡¦ç†ä¸­
        job_info.update({
            "status": ImportStatus.PROCESSING,
            "progress": 0.1,
            "updated_at": datetime.utcnow()
        })
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        storage_provider = get_storage_provider()
        
        # TODO: å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†
        # ç¾åœ¨ã¯ãƒ€ãƒŸãƒ¼å®Ÿè£…
        logger.info(f"Starting file processing for import {import_id}, media_id: {media_id}")
        
        # ãƒ€ãƒŸãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ storage_provider.download_file() ã‚’ä½¿ç”¨ï¼‰
        file_data = b"Dummy file content for testing"
        filename = f"imported_file_{media_id}.txt"
        
        job_info.update({
            "progress": 0.3,
            "updated_at": datetime.utcnow()
        })
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        extraction_result = await file_processor.extract_text_from_file(
            file_data, filename
        )
        
        job_info.update({
            "progress": 0.6,
            "updated_at": datetime.utcnow()
        })
        
        # AIã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        title = f"Imported from {filename}"
        if auto_title and extraction_result.get('text'):
            try:
                ai_service = AIService()
                title = await ai_service.generate_title(extraction_result['text'][:500])
                logger.info(f"AI title generated for import {import_id}: {title}")
            except Exception as e:
                logger.warning(f"AI title generation failed for import {import_id}: {e}")
        
        job_info.update({
            "progress": 0.8,
            "updated_at": datetime.utcnow()
        })
        
        # ãƒãƒ¼ãƒˆä½œæˆï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸å¯¾å¿œï¼‰
        note_id = f"import_{import_id}"
        extracted_text = extraction_result['text']
        
        # è‡ªå‹•ãƒšãƒ¼ã‚¸åˆ†å‰²å‡¦ç†
        pages = []
        if auto_split and len(extracted_text) > 2000:
            # 2000æ–‡å­—ã”ã¨ã«åˆ†å‰²
            text_chunks = [extracted_text[i:i+2000] for i in range(0, len(extracted_text), 2000)]
            for i, chunk in enumerate(text_chunks):
                pages.append({
                    "page_number": i + 1,
                    "text": chunk,
                    "text_length": len(chunk)
                })
        else:
            pages.append({
                "page_number": 1,
                "text": extracted_text,
                "text_length": len(extracted_text)
            })
        
        # å‡¦ç†å®Œäº†
        job_info.update({
            "status": ImportStatus.COMPLETED,
            "progress": 1.0,
            "note_id": note_id,
            "title": title,
            "total_pages": len(pages),
            "pages": pages,
            "extracted_text_length": len(extracted_text),
            "source_metadata": extraction_result.get('metadata', {}),
            "extraction_metadata": {
                "source_type": extraction_result.get('source_type'),
                "extraction_time": datetime.utcnow().isoformat()
            },
            "processing_time": (datetime.utcnow() - job_info["created_at"]).total_seconds(),
            "updated_at": datetime.utcnow(),
            "completed_at": datetime.utcnow()
        })
        
        logger.info(f"File import completed successfully: {import_id}")
        
    except FileProcessorError as e:
        logger.error(f"File import error for {import_id}: {e}")
        job_info.update({
            "status": ImportStatus.FAILED,
            "error_message": str(e),
            "error_code": "FILE_IMPORT_ERROR",
            "updated_at": datetime.utcnow(),
            "completed_at": datetime.utcnow()
        })
        
    except Exception as e:
        logger.error(f"Unexpected error in file import {import_id}: {e}")
        job_info.update({
            "status": ImportStatus.FAILED,
            "error_message": "äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            "error_code": "INTERNAL_ERROR",
            "updated_at": datetime.utcnow(),
            "completed_at": datetime.utcnow()
        }) 