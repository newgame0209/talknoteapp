"""
Google Cloud Storage (GCS) ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å®Ÿè£…
æœ¬ç•ªç’°å¢ƒç”¨ã®GCSãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
"""
import os
import json
import asyncio
from typing import Dict, List, Optional, BinaryIO, Union
from uuid import UUID, uuid4
from datetime import datetime, timedelta
from pathlib import Path
import tempfile

from google.cloud import storage
from google.cloud.exceptions import NotFound
from google.api_core.exceptions import GoogleAPIError

from app.core.settings import settings
from app.providers.storage.base import StorageProvider


class GCSStorageProvider(StorageProvider):
    """
    Google Cloud Storage (GCS) ã‚’ä½¿ç”¨ã—ãŸã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
    æœ¬ç•ªç’°å¢ƒç”¨
    """
    
    def __init__(self):
        """
        GCSã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–
        """
        # GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.client = storage.Client()
        
        # ãƒã‚±ãƒƒãƒˆåã®è¨­å®š
        self.bucket_name = settings.GCS_BUCKET_NAME
        self.bucket = self.client.bucket(self.bucket_name)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ™‚ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.temp_dir = Path(tempfile.gettempdir()) / "talknote_metadata"
        self.temp_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_blob_path(self, user_id: str, media_id: str, ext: str = None) -> str:
        """
        GCSå†…ã®Blobãƒ‘ã‚¹ã‚’å–å¾—
        """
        if ext:
            return f"{user_id}/{media_id}.{ext}"
        else:
            return f"{user_id}/{media_id}"
    
    def _get_chunk_blob_path(self, user_id: str, media_id: str, chunk_index: int) -> str:
        """
        ãƒãƒ£ãƒ³ã‚¯ç”¨ã®Blobãƒ‘ã‚¹ã‚’å–å¾—
        """
        return f"{user_id}/{media_id}/chunks/chunk_{chunk_index:04d}.bin"
    
    def _get_metadata_blob_path(self, user_id: str, media_id: str) -> str:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ã®Blobãƒ‘ã‚¹ã‚’å–å¾—
        """
        return f"{user_id}/{media_id}/metadata.json"
    
    async def _save_metadata_to_gcs(self, user_id: str, media_id: str, metadata: Dict) -> None:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ä¿å­˜
        """
        metadata_path = self._get_metadata_blob_path(user_id, media_id)
        blob = self.bucket.blob(metadata_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã«å¤‰æ›
        metadata_json = json.dumps(metadata, ensure_ascii=False)
        
        # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob.upload_from_string(metadata_json, content_type="application/json")
    
    async def _load_metadata_from_gcs(self, user_id: str, media_id: str) -> Dict:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’GCSã‹ã‚‰èª­ã¿è¾¼ã¿
        """
        metadata_path = self._get_metadata_blob_path(user_id, media_id)
        blob = self.bucket.blob(metadata_path)
        
        try:
            # GCSã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            metadata_json = blob.download_as_text()
            return json.loads(metadata_json)
        except NotFound:
            return {}
        except Exception as e:
            print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    async def _update_metadata(self, user_id: str, media_id: str, updates: Dict) -> Dict:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
        """
        metadata = await self._load_metadata_from_gcs(user_id, media_id)
        metadata.update(updates)
        await self._save_metadata_to_gcs(user_id, media_id, metadata)
        return metadata
    
    async def generate_upload_url(
        self, 
        media_id: Union[str, UUID], 
        file_type: str, 
        file_size: int,
        user_id: str,
        expires_in: int = 3600
    ) -> Dict:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®ç½²åä»˜ãURLã‚’ç”Ÿæˆ
        """
        media_id_str = str(media_id)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã®å–å¾—
        ext = self._get_extension_from_mimetype(file_type)
        
        # Blobãƒ‘ã‚¹ã®ç”Ÿæˆ
        blob_path = self._get_blob_path(user_id, media_id_str, ext)
        blob = self.bucket.blob(blob_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        metadata = {
            "media_id": media_id_str,
            "user_id": user_id,
            "file_type": file_type,
            "file_size": file_size,
            "status": "pending",
            "progress": 0.0,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "blob_path": blob_path
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        await self._save_metadata_to_gcs(user_id, media_id_str, metadata)
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ã‹ã©ã†ã‹ã®åˆ¤æ–­
        chunk_upload_enabled = file_size > settings.MAX_DIRECT_UPLOAD_SIZE
        
        if chunk_upload_enabled:
            # ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªå ´åˆ
            return {
                "media_id": media_id_str,
                "upload_url": None,  # ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ã¯ç›´æ¥URLã¯ä½¿ç”¨ã—ãªã„
                "chunk_upload_enabled": True,
                "max_chunk_size": settings.MAX_CHUNK_SIZE
            }
        else:
            # ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å ´åˆã€ç½²åä»˜ãURLã‚’ç”Ÿæˆ
            expiration = datetime.now() + timedelta(seconds=expires_in)
            signed_url = blob.generate_signed_url(
                version="v4",
                expiration=expiration,
                method="PUT",
                content_type=file_type
            )
            
            return {
                "media_id": media_id_str,
                "upload_url": signed_url,
                "chunk_upload_enabled": False,
                "max_chunk_size": settings.MAX_CHUNK_SIZE
            }
    
    async def upload_chunk(
        self,
        media_id: Union[str, UUID],
        chunk_index: int,
        total_chunks: int,
        chunk_data: BinaryIO,
        user_id: str
    ) -> Dict:
        """
        ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        """
        media_id_str = str(media_id)
        
        # ãƒãƒ£ãƒ³ã‚¯ç”¨Blobãƒ‘ã‚¹ã®ç”Ÿæˆ
        chunk_blob_path = self._get_chunk_blob_path(user_id, media_id_str, chunk_index)
        chunk_blob = self.bucket.blob(chunk_blob_path)
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        chunk_content = chunk_data.read()
        chunk_size = len(chunk_content)
        
        # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        chunk_blob.upload_from_string(chunk_content)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
        metadata = await self._load_metadata_from_gcs(user_id, media_id_str)
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã®è¿½åŠ 
        if "chunks" not in metadata:
            metadata["chunks"] = {}
        
        metadata["chunks"][str(chunk_index)] = {
            "size": chunk_size,
            "path": chunk_blob_path,
            "uploaded_at": datetime.now().isoformat()
        }
        
        metadata["total_chunks"] = total_chunks
        metadata["updated_at"] = datetime.now().isoformat()
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é€²æ—ã®è¨ˆç®—
        uploaded_chunks = len(metadata["chunks"])
        metadata["upload_progress"] = uploaded_chunks / total_chunks
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        await self._save_metadata_to_gcs(user_id, media_id_str, metadata)
        
        return {
            "media_id": media_id_str,
            "chunk_index": chunk_index,
            "received_bytes": chunk_size,
            "status": "success"
        }
    
    async def complete_upload(
        self,
        media_id: Union[str, UUID],
        total_chunks: int,
        total_size: int,
        md5_hash: Optional[str],
        user_id: str
    ) -> Dict:
        """
        ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å®Œäº†å‡¦ç†
        """
        media_id_str = str(media_id)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        metadata = await self._load_metadata_from_gcs(user_id, media_id_str)
        
        # ãƒãƒ£ãƒ³ã‚¯æ•°ã®ç¢ºèª
        if "chunks" not in metadata or len(metadata["chunks"]) != total_chunks:
            return {
                "media_id": media_id_str,
                "status": "error",
                "progress": 0.0,
                "error": f"ãƒãƒ£ãƒ³ã‚¯æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚æœŸå¾…: {total_chunks}, å®Ÿéš›: {len(metadata.get('chunks', {}))}"
            }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã®å–å¾—
        ext = self._get_extension_from_mimetype(metadata.get("file_type", "application/octet-stream"))
        
        # æœ€çµ‚çš„ãªBlobãƒ‘ã‚¹
        final_blob_path = self._get_blob_path(user_id, media_id_str, ext)
        final_blob = self.bucket.blob(final_blob_path)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_path = temp_file.name
            
            # ãƒãƒ£ãƒ³ã‚¯ã®çµåˆ
            for i in range(total_chunks):
                chunk_info = metadata["chunks"].get(str(i))
                if not chunk_info:
                    return {
                        "media_id": media_id_str,
                        "status": "error",
                        "progress": 0.0,
                        "error": f"ãƒãƒ£ãƒ³ã‚¯ {i} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                    }
                
                chunk_blob_path = chunk_info["path"]
                chunk_blob = self.bucket.blob(chunk_blob_path)
                
                # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
                chunk_data = chunk_blob.download_as_bytes()
                temp_file.write(chunk_data)
        
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            final_blob.upload_from_filename(temp_path)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            os.unlink(temp_path)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
            metadata["status"] = "processing"
            metadata["progress"] = 0.0
            metadata["blob_path"] = final_blob_path
            metadata["updated_at"] = datetime.now().isoformat()
            
            if md5_hash:
                metadata["md5_hash"] = md5_hash
            
            await self._save_metadata_to_gcs(user_id, media_id_str, metadata)
            
            # éåŒæœŸã§å‡¦ç†ã‚’é–‹å§‹ï¼ˆã“ã“ã§ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            asyncio.create_task(self._process_media(user_id, media_id_str))
            
            return {
                "media_id": media_id_str,
                "status": "processing",
                "progress": 0.0
            }
        except Exception as e:
            return {
                "media_id": media_id_str,
                "status": "error",
                "progress": 0.0,
                "error": f"ãƒ•ã‚¡ã‚¤ãƒ«çµåˆã‚¨ãƒ©ãƒ¼: {str(e)}"
            }
    
    async def _process_media(self, user_id: str, media_id: str) -> None:
        """
        ãƒ¡ãƒ‡ã‚£ã‚¢å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        å®Ÿéš›ã®ç’°å¢ƒã§ã¯ã€ã“ã“ã§STTã‚„OCRå‡¦ç†ã‚’è¡Œã†
        """
        # å‡¦ç†æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        total_steps = 5
        
        for step in range(1, total_steps + 1):
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
            metadata = await self._load_metadata_from_gcs(user_id, media_id)
            metadata["progress"] = step / total_steps
            metadata["updated_at"] = datetime.now().isoformat()
            await self._save_metadata_to_gcs(user_id, media_id, metadata)
            
            # å‡¦ç†æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            await asyncio.sleep(2)
        
        # å‡¦ç†å®Œäº†
        metadata = await self._load_metadata_from_gcs(user_id, media_id)
        metadata["status"] = "completed"
        metadata["progress"] = 1.0
        metadata["updated_at"] = datetime.now().isoformat()
        metadata["result"] = {
            "transcript": "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡å­—èµ·ã“ã—çµæœã§ã™ã€‚",
            "duration": 5.0,
            "language": "ja-JP"
        }
        await self._save_metadata_to_gcs(user_id, media_id, metadata)
    
    async def get_media_status(
        self,
        media_id: Union[str, UUID],
        user_id: str
    ) -> Dict:
        """
        ãƒ¡ãƒ‡ã‚£ã‚¢å‡¦ç†çŠ¶æ³ã‚’å–å¾—
        """
        media_id_str = str(media_id)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        metadata = await self._load_metadata_from_gcs(user_id, media_id_str)
        
        if not metadata:
            return {
                "media_id": media_id_str,
                "status": "error",
                "progress": 0.0,
                "error": "ãƒ¡ãƒ‡ã‚£ã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                "result": None
            }
        
        return {
            "media_id": media_id_str,
            "status": metadata.get("status", "unknown"),
            "progress": metadata.get("progress", 0.0),
            "error": metadata.get("error"),
            "result": metadata.get("result")
        }
    
    async def get_file_url(
        self,
        media_id: Union[str, UUID],
        user_id: str,
        expires_in: int = 3600
    ) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’å–å¾—
        """
        media_id_str = str(media_id)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        metadata = await self._load_metadata_from_gcs(user_id, media_id_str)
        
        if not metadata or "blob_path" not in metadata:
            raise FileNotFoundError(f"ãƒ¡ãƒ‡ã‚£ã‚¢ {media_id_str} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # Blobã®å–å¾—
        blob_path = metadata["blob_path"]
        blob = self.bucket.blob(blob_path)
        
        # ç½²åä»˜ãURLã®ç”Ÿæˆ
        expiration = datetime.now() + timedelta(seconds=expires_in)
        signed_url = blob.generate_signed_url(
            version="v4",
            expiration=expiration,
            method="GET"
        )
        
        return signed_url
    
    async def delete_file(
        self,
        media_id: Union[str, UUID],
        user_id: str
    ) -> bool:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        """
        media_id_str = str(media_id)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        metadata = await self._load_metadata_from_gcs(user_id, media_id_str)
        
        if not metadata:
            return False
        
        try:
            # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            if "blob_path" in metadata:
                blob_path = metadata["blob_path"]
                blob = self.bucket.blob(blob_path)
                blob.delete()
            
            # ãƒãƒ£ãƒ³ã‚¯ã®å‰Šé™¤
            if "chunks" in metadata:
                for chunk_index, chunk_info in metadata["chunks"].items():
                    chunk_path = chunk_info["path"]
                    chunk_blob = self.bucket.blob(chunk_path)
                    chunk_blob.delete()
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
            metadata_path = self._get_metadata_blob_path(user_id, media_id_str)
            metadata_blob = self.bucket.blob(metadata_path)
            metadata_blob.delete()
            
            return True
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _get_extension_from_mimetype(self, mimetype: str) -> str:
        """
        MIMEã‚¿ã‚¤ãƒ—ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’å–å¾—
        """
        mimetype_map = {
            "audio/wav": "wav",
            "audio/x-wav": "wav",
            "audio/wave": "wav",
            "audio/mp3": "mp3",
            "audio/mpeg": "mp3",
            "audio/m4a": "m4a",
            "audio/mp4": "m4a",
            "audio/aac": "aac",
            "audio/ogg": "ogg",
            "audio/webm": "webm",
            "application/pdf": "pdf",
            "image/jpeg": "jpg",
            "image/png": "png",
            "application/octet-stream": "bin"
        }
        
        return mimetype_map.get(mimetype.lower(), "bin")

    async def upload_file(self, media_id: str, file: BinaryIO, filename: str, content_type: str, user_id: str) -> dict:
        """
        FormDataã§å—ã‘å–ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ä¿å­˜
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã®å–å¾—
        ext = self._get_extension_from_mimetype(content_type)
        blob_path = self._get_blob_path(user_id, media_id, ext)
        blob = self.bucket.blob(blob_path)
        # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob.upload_from_file(file, content_type=content_type)
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        metadata = {
            "media_id": media_id,
            "user_id": user_id,
            "file_type": content_type,
            "status": "processing",
            "progress": 0.0,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "blob_path": blob_path
        }
        await self._save_metadata_to_gcs(user_id, media_id, metadata)
        # éåŒæœŸã§å‡¦ç†ã‚’é–‹å§‹ï¼ˆã“ã“ã§ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        asyncio.create_task(self._process_media(user_id, media_id))
        return {"status": "success", "media_id": media_id, "blob_path": blob_path}

    # ğŸ†• å†™çœŸã‚¹ã‚­ãƒ£ãƒ³å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    def _get_photo_scan_blob_path(self, note_id: str, page_id: str) -> str:
        """
        å†™çœŸã‚¹ã‚­ãƒ£ãƒ³ç”¨ã®Blobãƒ‘ã‚¹ã‚’å–å¾—
        å½¢å¼: {note_id}/{page_id}.jpg
        """
        return f"{note_id}/{page_id}.jpg"
    
    async def upload_photo_scan_image(
        self, 
        note_id: str, 
        page_id: str, 
        image_data: bytes, 
        user_id: str
    ) -> dict:
        """
        å†™çœŸã‚¹ã‚­ãƒ£ãƒ³ç”»åƒã‚’GCSã«ä¿å­˜
        è¤‡æ•°ãƒšãƒ¼ã‚¸å¯¾å¿œã®ãŸã‚ note_id/page_id.jpg å½¢å¼ã§ä¿å­˜
        
        Args:
            note_id: ãƒãƒ¼ãƒˆID
            page_id: ãƒšãƒ¼ã‚¸ID
            image_data: ç”»åƒã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            
        Returns:
            ä¿å­˜çµæœã®è¾æ›¸
        """
        try:
            # å†™çœŸã‚¹ã‚­ãƒ£ãƒ³ç”¨Blobãƒ‘ã‚¹ã®ç”Ÿæˆ
            blob_path = self._get_photo_scan_blob_path(note_id, page_id)
            blob = self.bucket.blob(blob_path)
            
            # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            blob.upload_from_string(
                image_data, 
                content_type="image/jpeg"
            )
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            metadata = {
                "note_id": note_id,
                "page_id": page_id,
                "user_id": user_id,
                "file_type": "image/jpeg",
                "status": "completed",
                "blob_path": blob_path,
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "storage_type": "photo_scan"
            }
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆå†™çœŸã‚¹ã‚­ãƒ£ãƒ³ç”¨ï¼‰
            await self._save_photo_scan_metadata(note_id, page_id, metadata)
            
            return {
                "status": "success",
                "note_id": note_id,
                "page_id": page_id,
                "blob_path": blob_path,
                "public_url": f"gs://{self.bucket_name}/{blob_path}"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "note_id": note_id,
                "page_id": page_id,
                "error": f"ç”»åƒä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}"
            }
    
    async def _save_photo_scan_metadata(self, note_id: str, page_id: str, metadata: Dict) -> None:
        """
        å†™çœŸã‚¹ã‚­ãƒ£ãƒ³ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ä¿å­˜
        """
        metadata_path = f"{note_id}/{page_id}_metadata.json"
        blob = self.bucket.blob(metadata_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã«å¤‰æ›
        metadata_json = json.dumps(metadata, ensure_ascii=False)
        
        # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob.upload_from_string(metadata_json, content_type="application/json")
    
    async def get_photo_scan_image_url(
        self,
        note_id: str,
        page_id: str,
        expires_in: int = 3600
    ) -> str:
        """
        å†™çœŸã‚¹ã‚­ãƒ£ãƒ³ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’å–å¾—
        """
        blob_path = self._get_photo_scan_blob_path(note_id, page_id)
        blob = self.bucket.blob(blob_path)
        
        # ç½²åä»˜ãURLã®ç”Ÿæˆ
        expiration = datetime.now() + timedelta(seconds=expires_in)
        signed_url = blob.generate_signed_url(
            version="v4",
            expiration=expiration,
            method="GET"
        )
        
        return signed_url
    
    async def delete_photo_scan_images(self, note_id: str) -> bool:
        """
        å†™çœŸã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒˆã®å…¨ç”»åƒã‚’å‰Šé™¤
        """
        try:
            # note_id/ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            blobs = self.bucket.list_blobs(prefix=f"{note_id}/")
            
            for blob in blobs:
                blob.delete()
            
            return True
            
        except Exception as e:
            print(f"å†™çœŸã‚¹ã‚­ãƒ£ãƒ³ç”»åƒå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False