"""
URL Import Service

URLï¼ˆWebãƒšãƒ¼ã‚¸ãƒ»YouTubeç­‰ï¼‰ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹å‡¦ç†ã‚’ç®¡ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹å±¤ã§ã™ã€‚
è¤‡æ•°ã®URLå½¢å¼ã«å¯¾å¿œã—ã€çµ±ä¸€çš„ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import logging
from typing import Dict, Optional, Any, List
import re
from urllib.parse import urlparse, parse_qs
import asyncio

# ğŸ†• Feature Flagå¯¾å¿œã®ãŸã‚ã®import
from app.core.settings import settings

try:
    import requests
    from requests.adapters import HTTPAdapter
    from requests.packages.urllib3.util.retry import Retry
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    requests = None

try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False
    BeautifulSoup = None

try:
    import yt_dlp
    YTDLP_AVAILABLE = True
except ImportError:
    YTDLP_AVAILABLE = False
    yt_dlp = None

logger = logging.getLogger(__name__)


class URLImportError(Exception):
    """URL ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼"""
    pass


class URLImporter:
    """URL ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """URL ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–"""
        self.supported_domains = self._get_supported_domains()
        self.session = self._create_session()
        logger.info(f"URLImporter initialized. Supported domains: {list(self.supported_domains.keys())}")
    
    def _get_supported_domains(self) -> Dict[str, str]:
        """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å–å¾—"""
        domains = {}
        
        # ä¸€èˆ¬çš„ãªWebãƒšãƒ¼ã‚¸
        if REQUESTS_AVAILABLE and BEAUTIFULSOUP_AVAILABLE:
            domains.update({
                'general': 'webpage',
                'news': 'webpage',
                'blog': 'webpage',
                'wiki': 'webpage'
            })
        
        # YouTube
        if YTDLP_AVAILABLE:
            domains.update({
                'youtube.com': 'youtube',
                'youtu.be': 'youtube',
                'm.youtube.com': 'youtube'
            })
        
        return domains
    
    def _create_session(self) -> Optional[requests.Session]:
        """HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        if not REQUESTS_AVAILABLE:
            return None
        
        session = requests.Session()
        
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # User-Agentè¨­å®š
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        return session
    
    async def extract_text_from_url(
        self,
        url: str,
        extract_options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        
        Args:
            url: æŠ½å‡ºå¯¾è±¡ã®URL
            extract_options: æŠ½å‡ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
            
        Returns:
            Dict[str, Any]: æŠ½å‡ºçµæœ
            - text: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            - title: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
            - metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            - source_type: ã‚½ãƒ¼ã‚¹ã®ç¨®é¡
            
        Raises:
            URLImportError: URLå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        if not url:
            raise URLImportError("URL is required")
        
        # URL ã®æ¤œè¨¼
        if not self._is_valid_url(url):
            raise URLImportError(f"Invalid URL format: {url}")
        
        # URL ã®ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        url_type = self._get_url_type(url)
        
        try:
            # URL ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å‡¦ç†ã‚’å®Ÿè¡Œ
            if url_type == 'youtube':
                return await self._extract_from_youtube(url, extract_options)
            elif url_type == 'webpage':
                return await self._extract_from_webpage(url, extract_options)
            else:
                raise URLImportError(f"Unsupported URL type: {url_type}")
                
        except URLImportError:
            # URLImportErrorã¯ãã®ã¾ã¾å†ç™ºç”Ÿ
            raise
        except Exception as e:
            logger.error(f"Unexpected error in URL processing: {e}")
            raise URLImportError(f"Unexpected error in URL processing: {e}")
    
    def _is_valid_url(self, url: str) -> bool:
        """URL ã®å½¢å¼ã‚’æ¤œè¨¼"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False
    
    def _get_url_type(self, url: str) -> str:
        """URL ã®ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # YouTube ã®åˆ¤å®š
        if any(youtube_domain in domain for youtube_domain in ['youtube.com', 'youtu.be', 'm.youtube.com']):
            return 'youtube'
        
        # ä¸€èˆ¬çš„ãª Web ãƒšãƒ¼ã‚¸
        return 'webpage'
    
    async def _extract_from_youtube(self, url: str, extract_options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """YouTube URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        if not YTDLP_AVAILABLE:
            raise URLImportError("yt-dlp is not available")
        
        try:
            # yt-dlp ã®è¨­å®š
            ydl_opts = {
                'quiet': True,
                'no_warnings': True,
                'writesubtitles': True,
                'writeautomaticsub': True,
                'subtitleslangs': ['ja', 'en'],
                'skip_download': True,
                'extract_flat': False,
            }
            
            # å‹•ç”»æƒ…å ±ã®å–å¾—
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜æ–‡ã®å–å¾—
            title = info.get('title', '')
            description = info.get('description', '')
            uploader = info.get('uploader', '')
            upload_date = info.get('upload_date', '')
            view_count = info.get('view_count', 0)
            duration = info.get('duration', 0)
            
            # ğŸ”§ å¼·åŒ–: YouTubeå­—å¹•å–å¾—å‡¦ç†ã®æ”¹å–„
            subtitles_text = ''
            logger.info(f"å­—å¹•å–å¾—é–‹å§‹: {url}")
            
            # å­—å¹•ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ãƒ­ã‚°
            subtitles_available = info.get('subtitles', {})
            auto_captions_available = info.get('automatic_captions', {})
            
            logger.info(f"åˆ©ç”¨å¯èƒ½ãªå­—å¹•: {list(subtitles_available.keys())}")
            logger.info(f"åˆ©ç”¨å¯èƒ½ãªè‡ªå‹•å­—å¹•: {list(auto_captions_available.keys())}")
            
            if subtitles_available or auto_captions_available:
                # ğŸš¨ CRITICAL: å­—å¹•å–å¾—ã®å„ªå…ˆé †ä½ã‚’æ”¹å–„
                subtitle_langs = ['ja', 'ja-JP', 'en', 'en-US', 'en-GB']
                subtitle_entries = None
                selected_lang = None
                
                # æ‰‹å‹•å­—å¹•ã‚’å„ªå…ˆ
                for lang in subtitle_langs:
                    if lang in subtitles_available:
                        subtitle_entries = subtitles_available[lang]
                        selected_lang = lang
                        logger.info(f"æ‰‹å‹•å­—å¹•ã‚’é¸æŠ: {lang}")
                        break
                
                # æ‰‹å‹•å­—å¹•ãŒãªã„å ´åˆã¯è‡ªå‹•å­—å¹•ã‚’ä½¿ç”¨
                if not subtitle_entries:
                    for lang in subtitle_langs:
                        if lang in auto_captions_available:
                            subtitle_entries = auto_captions_available[lang]
                            selected_lang = lang
                            logger.info(f"è‡ªå‹•å­—å¹•ã‚’é¸æŠ: {lang}")
                            break
                
                # å­—å¹•ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ãƒ­ã‚°
                if subtitle_entries:
                    logger.info(f"å­—å¹•ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(subtitle_entries)}")
                    for i, entry in enumerate(subtitle_entries[:3]):  # æœ€åˆã®3ã¤ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ­ã‚°
                        logger.info(f"å­—å¹•ã‚¨ãƒ³ãƒˆãƒª {i}: {entry}")
                    
                    subtitles_text = self._extract_subtitle_text(subtitle_entries)
                    if subtitles_text:
                        logger.info(f"âœ… å­—å¹•å–å¾—æˆåŠŸ: {len(subtitles_text)} æ–‡å­— (è¨€èª: {selected_lang})")
                    else:
                        logger.warning(f"âš ï¸ å­—å¹•ã‚¨ãƒ³ãƒˆãƒªã¯ã‚ã‚‹ãŒã€ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«å¤±æ•—")
                else:
                    logger.warning(f"âŒ å¯¾å¿œè¨€èªã®å­—å¹•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {subtitle_langs}")
            else:
                logger.info(f"â„¹ï¸ å­—å¹•ãƒ‡ãƒ¼ã‚¿ãªã—: {url}")
            
            # ãƒ†ã‚­ã‚¹ãƒˆã®çµåˆ
            text_parts = []
            if title:
                text_parts.append(f"ã‚¿ã‚¤ãƒˆãƒ«: {title}")
            if description:
                text_parts.append(f"èª¬æ˜: {description}")
            if subtitles_text:
                text_parts.append(f"å­—å¹•: {subtitles_text}")
            
            full_text = '\n\n'.join(text_parts)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            metadata = {
                'title': title,
                'description': description,
                'uploader': uploader,
                'upload_date': upload_date,
                'view_count': view_count,
                'duration': duration,
                'url': url,
                'video_id': info.get('id', ''),
                'has_subtitles': bool(subtitles_text),
                'text_length': len(full_text)
            }
            
            logger.info(f"YouTube video processed. Title: {title}, Duration: {duration}s, Text length: {len(full_text)}")
            
            return {
                'text': full_text,
                'title': title,
                'metadata': metadata,
                'source_type': 'youtube'
            }
            
        except Exception as e:
            logger.error(f"Error processing YouTube URL {url}: {e}")
            raise URLImportError(f"Failed to process YouTube URL: {e}")
    
    def _extract_subtitle_text(self, subtitle_entries: List[Dict]) -> str:
        """å­—å¹•ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            logger.info(f"å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹: {len(subtitle_entries)} ã‚¨ãƒ³ãƒˆãƒª")
            
            # ğŸ”§ å¼·åŒ–: è¤‡æ•°ã®å­—å¹•å½¢å¼ã«å¯¾å¿œ
            subtitle_url = None
            subtitle_format = None
            
            # å„ªå…ˆé †ä½: VTT > SRT > TTML > ãã®ä»–
            format_priority = ['vtt', 'srt', 'ttml', 'srv3', 'srv2', 'srv1']
            
            for fmt in format_priority:
                for entry in subtitle_entries:
                    if entry.get('ext') == fmt:
                        subtitle_url = entry.get('url')
                        subtitle_format = fmt
                        logger.info(f"å­—å¹•å½¢å¼é¸æŠ: {fmt}, URL: {subtitle_url}")
                        break
                if subtitle_url:
                    break
            
            if not subtitle_url:
                logger.warning("å­—å¹•URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return ''
            
            # å­—å¹•ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            logger.info(f"å­—å¹•ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {subtitle_url}")
            response = self.session.get(subtitle_url, timeout=30)
            response.raise_for_status()
            
            # ğŸ”§ å¼·åŒ–: å½¢å¼ã«å¿œã˜ãŸè§£æå‡¦ç†
            if subtitle_format in ['vtt', 'srt']:
                subtitle_text = self._parse_vtt_srt_subtitles(response.text)
            elif subtitle_format == 'ttml':
                subtitle_text = self._parse_ttml_subtitles(response.text)
            else:
                # ãã®ä»–ã®å½¢å¼ã¯åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                subtitle_text = self._parse_generic_subtitles(response.text)
            
            logger.info(f"âœ… å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(subtitle_text)} æ–‡å­—")
            return subtitle_text
            
        except Exception as e:
            logger.error(f"âŒ å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return ''
    
    def _parse_vtt_srt_subtitles(self, subtitle_content: str) -> str:
        """VTT/SRTå½¢å¼ã®å­—å¹•ã‚’è§£æ"""
        lines = subtitle_content.split('\n')
        subtitle_text = []
        
        for line in lines:
            line = line.strip()
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if '-->' in line or line.startswith('WEBVTT') or line.startswith('NOTE') or line.isdigit():
                continue
            # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not line:
                continue
            # HTMLã‚¿ã‚°ã‚’é™¤å»
            clean_line = re.sub(r'<[^>]+>', '', line)
            # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
            clean_line = re.sub(r'&[a-zA-Z0-9#]+;', '', clean_line)
            if clean_line:
                subtitle_text.append(clean_line)
        
        return ' '.join(subtitle_text)
    
    def _parse_ttml_subtitles(self, subtitle_content: str) -> str:
        """TTMLå½¢å¼ã®å­—å¹•ã‚’è§£æ"""
        try:
            # åŸºæœ¬çš„ãªXMLè§£æï¼ˆBeautifulSoupã‚’ä½¿ç”¨ï¼‰
            if not BEAUTIFULSOUP_AVAILABLE:
                return self._parse_generic_subtitles(subtitle_content)
            
            soup = BeautifulSoup(subtitle_content, 'xml')
            text_elements = soup.find_all('p')
            
            subtitle_text = []
            for element in text_elements:
                text = element.get_text().strip()
                if text:
                    subtitle_text.append(text)
            
            return ' '.join(subtitle_text)
            
        except Exception as e:
            logger.warning(f"TTMLè§£æã‚¨ãƒ©ãƒ¼ã€åŸºæœ¬è§£æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
            return self._parse_generic_subtitles(subtitle_content)
    
    def _parse_generic_subtitles(self, subtitle_content: str) -> str:
        """æ±ç”¨çš„ãªå­—å¹•ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        lines = subtitle_content.split('\n')
        subtitle_text = []
        
        for line in lines:
            line = line.strip()
            # æ˜ã‚‰ã‹ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if (not line or 
                '-->' in line or 
                line.startswith('<') or 
                line.startswith('WEBVTT') or 
                line.startswith('NOTE') or
                line.isdigit() or
                re.match(r'^\d{2}:\d{2}:\d{2}', line)):
                continue
            
            # HTMLã‚¿ã‚°ã¨ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
            clean_line = re.sub(r'<[^>]+>', '', line)
            clean_line = re.sub(r'&[a-zA-Z0-9#]+;', '', clean_line)
            
            if clean_line:
                subtitle_text.append(clean_line)
        
        return ' '.join(subtitle_text)
    
    async def _extract_from_webpage(self, url: str, extract_options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ä¸€èˆ¬çš„ãªWebãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        if not REQUESTS_AVAILABLE or not BEAUTIFULSOUP_AVAILABLE:
            raise URLImportError("requests and beautifulsoup4 are not available")
        
        try:
            # Webãƒšãƒ¼ã‚¸ã®å–å¾—
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è¨­å®š
            response.encoding = response.apparent_encoding
            
            # HTMLã®è§£æ
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
            title = ''
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            description = ''
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                description = meta_desc.get('content', '').strip()
            
            # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
                tag.decompose()
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡º
            main_content = self._extract_main_content(soup)
            
            # ãƒ†ã‚­ã‚¹ãƒˆã®æ¸…æ›¸
            clean_text = self._clean_text(main_content)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            metadata = {
                'title': title,
                'description': description,
                'url': url,
                'domain': urlparse(url).netloc,
                'text_length': len(clean_text),
                'content_type': 'webpage'
            }
            
            logger.info(f"Webpage processed. Title: {title}, Domain: {metadata['domain']}, Text length: {len(clean_text)}")
            
            return {
                'text': clean_text,
                'title': title,
                'metadata': metadata,
                'source_type': 'webpage'
            }
            
        except Exception as e:
            logger.error(f"Error processing webpage {url}: {e}")
            raise URLImportError(f"Failed to process webpage: {e}")
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å€™è£œã®ã‚¿ã‚°ã‚’å„ªå…ˆé †ä½ã§æ¤œç´¢
        content_selectors = [
            'article',
            'main',
            '[role="main"]',
            '.content',
            '.main-content',
            '.post-content',
            '.entry-content',
            '.article-content',
            'div.content',
            'div#content',
            'body'
        ]
        
        content = ''
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content = elements[0].get_text()
                break
        
        return content
    
    def _clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸…æ›¸"""
        if not text:
            return ''
        
        # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
        text = re.sub(r'\s+', ' ', text)
        
        # è¤‡æ•°ã®æ”¹è¡Œã‚’å˜ä¸€æ”¹è¡Œã«å¤‰æ›
        text = re.sub(r'\n+', '\n', text)
        
        # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»
        text = text.strip()
        
        return text
    
    def get_supported_domains(self) -> Dict[str, str]:
        """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self.supported_domains.copy()
    
    def is_url_supported(self, url: str) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸURLãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            url_type = self._get_url_type(url)
            return url_type in ['youtube', 'webpage']
        except Exception:
            return False
    
    def get_service_info(self) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚µãƒãƒ¼ãƒˆæƒ…å ±ã‚’å–å¾—"""
        return {
            'requests_available': REQUESTS_AVAILABLE,
            'beautifulsoup_available': BEAUTIFULSOUP_AVAILABLE,
            'ytdlp_available': YTDLP_AVAILABLE,
            'supported_domains': self.get_supported_domains(),
            'max_timeout': 30
        }


# ğŸ†• Phase 1: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½è¿½åŠ 
# æ—¢å­˜æ©Ÿèƒ½ã«å½±éŸ¿ã—ãªã„æ–°æ©Ÿèƒ½ã¨ã—ã¦å®Ÿè£…

def _split_into_chunks(text: str, max_chars: int = 2000) -> List[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ®µè½ã‚’ä¿æŒã—ãªãŒã‚‰æŒ‡å®šæ–‡å­—æ•°ã§åˆ†å‰²
    
    Args:
        text: åˆ†å‰²å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        max_chars: 1ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°
        
    Returns:
        List[str]: åˆ†å‰²ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    if len(text) <= max_chars:
        return [text]
    
    # æ®µè½ã§åˆ†å‰²ï¼ˆ\n\n ã¾ãŸã¯ \n ã§åŒºåˆ‡ã‚Šï¼‰
    paragraphs = re.split(r'\n\s*\n', text)
    if len(paragraphs) == 1:
        # æ®µè½åˆ†å‰²ãŒã§ããªã„å ´åˆã¯æ–‡ã§åˆ†å‰²
        paragraphs = re.split(r'[ã€‚ï¼ï¼Ÿ]\s*', text)
    
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue
            
        # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        if len(current_chunk + paragraph) <= max_chars:
            if current_chunk:
                current_chunk += "\n\n" + paragraph
            else:
                current_chunk = paragraph
        else:
            # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            if current_chunk:
                chunks.append(current_chunk)
            
            # æ®µè½ãŒ1ãƒãƒ£ãƒ³ã‚¯ã®åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
            if len(paragraph) > max_chars:
                # æ–‡å­—æ•°åˆ¶é™ã§å¼·åˆ¶åˆ†å‰²
                for i in range(0, len(paragraph), max_chars):
                    chunks.append(paragraph[i:i + max_chars])
                current_chunk = ""
            else:
                current_chunk = paragraph
    
    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
    if current_chunk:
        chunks.append(current_chunk)
    
    logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å®Œäº†: {len(text)}æ–‡å­— â†’ {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")
    return chunks


async def extract_text_with_chunking(
    url: str,
    extract_options: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    ğŸ†• Phase 1: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å¯¾å¿œã®URLæŠ½å‡ºæ©Ÿèƒ½
    Feature Flag IMPORT_SPLIT_ENABLED ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ä½¿ç”¨
    
    Args:
        url: æŠ½å‡ºå¯¾è±¡ã®URL
        extract_options: æŠ½å‡ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
        
    Returns:
        Dict[str, Any]: æŠ½å‡ºçµæœï¼ˆpagesé…åˆ—ä»˜ãï¼‰
        - text: å…¨ä½“ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰
        - title: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
        - metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        - source_type: ã‚½ãƒ¼ã‚¹ã®ç¨®é¡
        - pages: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã•ã‚ŒãŸãƒšãƒ¼ã‚¸é…åˆ—ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
    """
    # Feature Flag ãƒã‚§ãƒƒã‚¯
    if not settings.IMPORT_SPLIT_ENABLED:
        # å¾“æ¥ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ï¼ˆå®Œå…¨å¾Œæ–¹äº’æ›ï¼‰
        logger.info("IMPORT_SPLIT_ENABLED=False: å¾“æ¥ã®æŠ½å‡ºæ©Ÿèƒ½ã‚’ä½¿ç”¨")
        return await url_importer.extract_text_from_url(url, extract_options)
    
    logger.info(f"IMPORT_SPLIT_ENABLED=True: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½ã‚’ä½¿ç”¨ - URL: {url}")
    
    # æ—¢å­˜ã®æŠ½å‡ºæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    base_result = await url_importer.extract_text_from_url(url, extract_options)
    
    full_text = base_result.get('text', '')
    title = base_result.get('title', '')
    metadata = base_result.get('metadata', {})
    source_type = base_result.get('source_type', 'webpage')
    
    # ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒã‚§ãƒƒã‚¯
    if len(full_text) <= 2000:
        # 2000æ–‡å­—ä»¥ä¸‹ã®å ´åˆã¯å¾“æ¥é€šã‚Šï¼ˆ1ãƒšãƒ¼ã‚¸ï¼‰
        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆé•· {len(full_text)}æ–‡å­—: åˆ†å‰²ä¸è¦")
        pages = [{
            'page_number': 1,
            'text': full_text,
            'char_count': len(full_text)
        }]
    else:
        # 2000æ–‡å­—è¶…ã®å ´åˆã¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆé•· {len(full_text)}æ–‡å­—: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Ÿè¡Œ")
        chunks = _split_into_chunks(full_text, max_chars=2000)
        
        pages = []
        for i, chunk in enumerate(chunks, 1):
            pages.append({
                'page_number': i,
                'text': chunk,
                'char_count': len(chunk)
            })
        
        logger.info(f"ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº†: {len(chunks)}ãƒšãƒ¼ã‚¸ç”Ÿæˆ")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²æƒ…å ±ã‚’è¿½åŠ 
    metadata.update({
        'total_pages': len(pages),
        'chunked': len(pages) > 1,
        'original_length': len(full_text)
    })
    
    # çµæœã‚’è¿”ã™ï¼ˆæ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ + æ–°ã—ã„pagesé…åˆ—ï¼‰
    result = {
        'text': full_text,  # å¾Œæ–¹äº’æ›ç”¨
        'title': title,
        'metadata': metadata,
        'source_type': source_type,
        'pages': pages  # ğŸ†• æ–°æ©Ÿèƒ½
    }
    
    logger.info(f"æŠ½å‡ºå®Œäº†: ã‚¿ã‚¤ãƒˆãƒ«='{title}', ãƒšãƒ¼ã‚¸æ•°={len(pages)}")
    return result


# ã‚°ãƒ­ãƒ¼ãƒãƒ«URL ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
url_importer = URLImporter() 